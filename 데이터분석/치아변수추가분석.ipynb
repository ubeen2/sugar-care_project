{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1af67ed-4e77-486a-aa08-5493670f8a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_location = \"C:\\Windows\\Fonts\\malgun.ttf\"\n",
    "font_name = font_manager.FontProperties(fname=font_location).get_name()\n",
    "rc('font', family=font_name)\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0754024-a3c0-4b3d-bdee-3118fccafc84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31686, number of negative: 48314\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 982\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31729, number of negative: 48271\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 865\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31694, number of negative: 48306\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1134\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31742, number of negative: 48258\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004215 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 867\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31822, number of negative: 48178\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 894\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "… 5/10 done (last AUC=0.715)\n",
      "[LightGBM] [Info] Number of positive: 31777, number of negative: 48223\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 885\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31819, number of negative: 48181\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001448 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 873\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31666, number of negative: 48334\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003346 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1019\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 13\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31726, number of negative: 48274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1359\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31890, number of negative: 48110\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003872 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 986\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 11\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "… 10/10 done (last AUC=0.720)\n",
      "\n",
      "=== 빠른 테스트 결과 TOP 5 ===\n",
      "   trial   k                                               cols     AUC\n",
      "8      9  12  [혈청지오티(AST), 음주여부, 연령대코드(5세단위), 수축기혈압, 구강검진수검여...  0.7236\n",
      "3      4  11  [요단백, 음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, 시력(우...  0.7228\n",
      "7      8  13  [요단백, 음주여부, 연령대코드(5세단위), 시도코드, 구강검진수검여부, 수축기혈압...  0.7228\n",
      "2      3  13  [혈청지오티(AST), 음주여부, 연령대코드(5세단위), 시도코드, 수축기혈압, 시...  0.7220\n",
      "0      1  11  [음주여부, 연령대코드(5세단위), 성별코드, 수축기혈압, 구강검진수검여부, HDL...  0.7216\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from lightgbm import LGBMClassifier\n",
    "import random\n",
    "\n",
    "df = pd.read_csv(\"국민건강보험공단_건강검진정보_2024.CSV\", encoding=\"cp949\")\n",
    "df[\"고혈당_분석용\"] = (df[\"식전혈당(공복혈당)\"] >= 100).astype(int)\n",
    "\n",
    "df_work = df.sample(n=100000, random_state=42).copy()\n",
    "\n",
    "TARGET = \"고혈당_분석용\"\n",
    "ID_LIKE = [\"기준년도\", \"가입자일련번호\"]\n",
    "GLUCOSE_COLS = [\"식전혈당(공복혈당)\"]\n",
    "MISSING_THRESH = 0.30\n",
    "LOW_REL_TOP_K = 15\n",
    "N_TRIALS = 10          \n",
    "K_RANGE = (3, 5)\n",
    "\n",
    "ANCHORS = [\"연령대코드(5세단위)\", \"허리둘레\", \"수축기혈압\", \"이완기혈압\",\n",
    "           \"HDL콜레스테롤\", \"LDL콜레스테롤\", \"흡연상태\", \"음주여부\"]\n",
    "ANCHORS = [c for c in ANCHORS if c in df_work.columns]\n",
    "\n",
    "exclude_cols = set([TARGET] + ID_LIKE + [c for c in GLUCOSE_COLS if c in df_work.columns])\n",
    "feature_pool = [c for c in df_work.columns if c not in exclude_cols]\n",
    "\n",
    "\n",
    "missing_rate = df_work[feature_pool].isna().mean().sort_values()\n",
    "kept_by_missing = missing_rate[missing_rate <= MISSING_THRESH].index.tolist()\n",
    "\n",
    "def eval_random_combos(df, pool_cols, y_col, n_trials=10, k_range=(3,5),\n",
    "                       always_include=None, random_state=42, verbose_every=5):\n",
    "    rng = random.Random(random_state)\n",
    "    results = []\n",
    "    pool_cols = [c for c in pool_cols if c not in (always_include or [])]\n",
    "    y = df[y_col].astype(int)\n",
    "\n",
    "    for i in range(1, n_trials+1):\n",
    "        k = rng.randint(k_range[0], k_range[1])\n",
    "        subset = rng.sample(pool_cols, min(k, len(pool_cols)))\n",
    "        if always_include:\n",
    "            subset = list(set(subset + list(always_include)))\n",
    "\n",
    "        X = df[subset].copy()\n",
    "        for c in X.columns:\n",
    "            if X[c].nunique() <= 15:\n",
    "                X[c] = X[c].fillna(X[c].mode().iloc[0])\n",
    "            else:\n",
    "                X[c] = X[c].astype(float).fillna(X[c].median())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=i\n",
    "        )\n",
    "\n",
    "        neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "        scale = max(1.0, neg / max(1, pos))\n",
    "\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=100,     \n",
    "            learning_rate=0.1,    \n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=i,\n",
    "            class_weight={0:1, 1:scale},\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        proba = model.predict_proba(X_test)[:,1]\n",
    "        auc = roc_auc_score(y_test, proba)\n",
    "\n",
    "        results.append({\n",
    "            \"trial\": i,\n",
    "            \"k\": len(subset),\n",
    "            \"cols\": subset,\n",
    "            \"AUC\": round(auc, 4)\n",
    "        })\n",
    "\n",
    "        if verbose_every and i % verbose_every == 0:\n",
    "            print(f\"… {i}/{n_trials} done (last AUC={auc:.3f})\")\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=\"AUC\", ascending=False)\n",
    "\n",
    "res_test = eval_random_combos(\n",
    "    df=df_work,\n",
    "    pool_cols=kept_by_missing,\n",
    "    y_col=TARGET,\n",
    "    n_trials=N_TRIALS,\n",
    "    k_range=K_RANGE,\n",
    "    always_include=ANCHORS,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "print(\"\\n=== 빠른 테스트 결과 TOP 5 ===\")\n",
    "print(res_test.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68868af-d52d-4266-a5e0-f9ccdaa7695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rel_candidates = [\n",
    "    \"치아우식증유무\", \"치석\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b803ac73-2c18-420d-82be-0c76687706a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31732, number of negative: 48268\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31804, number of negative: 48196\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003708 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31828, number of negative: 48172\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31769, number of negative: 48231\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002041 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31783, number of negative: 48217\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003241 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "… 5/20 done (last AUC=0.717)\n",
      "[LightGBM] [Info] Number of positive: 31827, number of negative: 48173\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002296 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31699, number of negative: 48301\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Info] Number of positive: 31720, number of negative: 48280\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 847\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31756, number of negative: 48244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31878, number of negative: 48122\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002791 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "… 10/20 done (last AUC=0.720)\n",
      "[LightGBM] [Info] Number of positive: 31769, number of negative: 48231\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31814, number of negative: 48186\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31701, number of negative: 48299\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001610 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31794, number of negative: 48206\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31789, number of negative: 48211\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 843\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "… 15/20 done (last AUC=0.716)\n",
      "[LightGBM] [Info] Number of positive: 31766, number of negative: 48234\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31756, number of negative: 48244\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 848\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31840, number of negative: 48160\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 844\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31794, number of negative: 48206\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 843\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Info] Number of positive: 31690, number of negative: 48310\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 846\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "… 20/20 done (last AUC=0.720)\n",
      "\n",
      "=== 치과 변수 포함 (샘플 10만건, TOP 결과 10) ===\n",
      "    trial   k                                               cols     AUC\n",
      "16     17  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7265\n",
      "17     18  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7242\n",
      "11     12  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7228\n",
      "1       2  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7218\n",
      "12     13  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7213\n",
      "15     16  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7208\n",
      "13     14  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7206\n",
      "9      10  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7202\n",
      "19     20  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7200\n",
      "3       4  10  [음주여부, 연령대코드(5세단위), 수축기혈압, LDL콜레스테롤, HDL콜레스테롤,...  0.7198\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_sample = df_work.sample(n=100000, random_state=42)\n",
    "\n",
    "\n",
    "DENTAL_EXTRA = [c for c in [\"치아우식증유무\", \"치석\"] if c in df_sample.columns]\n",
    "\n",
    "res_with_dental = eval_random_combos(\n",
    "    df=df_sample,\n",
    "    pool_cols=low_rel_candidates,    \n",
    "    y_col=TARGET,\n",
    "    n_trials=20,                     \n",
    "    k_range=(5, 8),                 \n",
    "    always_include=ANCHORS + DENTAL_EXTRA,   \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "print(\"\\n=== 치과 변수 포함 (샘플 10만건, TOP 결과 10) ===\")\n",
    "print(res_with_dental.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067fb430-1d55-41a3-9ec6-9bf7507858da",
   "metadata": {},
   "source": [
    "### 치아변수 포함시 성능향상 = 치아와 혈당 관계 있음을 발견"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad567cc-eb1e-4616-b502-61696810119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 전부 NaN인 컬럼: ['결손치 유무', '치아마모증유무', '제3대구치(사랑니) 이상']\n",
      "\n",
      "=== RFE Top 10 피처 ===\n",
      "        feature  selected  ranking\n",
      "22        감마지티피      True        1\n",
      "21   혈청지피티(ALT)      True        1\n",
      "3   연령대코드(5세단위)      True        1\n",
      "20   혈청지오티(AST)      True        1\n",
      "5     체중(5kg단위)      True        1\n",
      "6          허리둘레      True        1\n",
      "17          혈색소      True        1\n",
      "16     LDL콜레스테롤      True        1\n",
      "11        수축기혈압      True        1\n",
      "13       총콜레스테롤      True        1\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2789\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 29\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.397462 -> initscore=-0.416049\n",
      "[LightGBM] [Info] Start training from score -0.416049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OWNER\\anaconda3\\envs\\prophet\\lib\\site-packages\\shap\\explainers\\_tree.py:583: UserWarning: LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SHAP Top 10 피처 ===\n",
      "        feature  shap_importance\n",
      "3   연령대코드(5세단위)         0.476083\n",
      "6          허리둘레         0.253645\n",
      "22        감마지티피         0.238442\n",
      "11        수축기혈압         0.132355\n",
      "21   혈청지피티(ALT)         0.110055\n",
      "20   혈청지오티(AST)         0.104809\n",
      "19      혈청크레아티닌         0.063159\n",
      "17          혈색소         0.053821\n",
      "14     트리글리세라이드         0.051776\n",
      "5     체중(5kg단위)         0.043946\n",
      "\n",
      "=== LightGBM 성능 ===\n",
      "ACC = 0.688\n",
      "F1  = 0.582\n",
      "AUC = 0.745\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFE\n",
    "from lightgbm import LGBMClassifier\n",
    "import shap\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"국민건강보험공단_건강검진정보_2024.CSV\", encoding=\"cp949\")\n",
    "\n",
    "\n",
    "df[\"고혈당_분석용\"] = (df[\"식전혈당(공복혈당)\"] >= 100).astype(int)\n",
    "\n",
    "target = \"고혈당_분석용\"\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "exclude_cols = [\"고혈당_분석용\", \"고혈당_서비스용\", \"식전혈당(공복혈당)\", \"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "\n",
    "null_only_cols = [c for c in X.columns if X[c].isna().all()]\n",
    "print(\" 전부 NaN인 컬럼:\", null_only_cols)\n",
    "\n",
    "X = X.drop(columns=null_only_cols)\n",
    "\n",
    "\n",
    "cat_cols = [\"성별코드\", \"흡연상태\", \"음주여부\"]\n",
    "X = pd.get_dummies(X, columns=[c for c in cat_cols if c in X.columns], drop_first=True)\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "y = y.astype(int)\n",
    "\n",
    "log_model = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "rfe = RFE(log_model, n_features_to_select=10)\n",
    "rfe.fit(X_scaled_df, y)\n",
    "\n",
    "rfe_support = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"selected\": rfe.support_,\n",
    "    \"ranking\": rfe.ranking_\n",
    "}).sort_values(\"ranking\")\n",
    "\n",
    "print(\"\\n=== RFE Top 10 피처 ===\")\n",
    "print(rfe_support[rfe_support[\"selected\"]])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled_df, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "lgb_model = LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(lgb_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"shap_importance\": shap_importance\n",
    "}).sort_values(\"shap_importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== SHAP Top 10 피처 ===\")\n",
    "print(shap_df.head(10))\n",
    "\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "y_proba = lgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"\\n=== LightGBM 성능 ===\")\n",
    "print(f\"ACC = {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1  = {f1_score(y_test, y_pred):.3f}\")\n",
    "print(f\"AUC = {roc_auc_score(y_test, y_proba):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49a978d5-a0cc-43fb-aa88-6d5567c9226b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Threshold  Precision  Recall     F1\n",
      "0       0.30      0.523   0.850  0.648\n",
      "1       0.40      0.569   0.727  0.639\n",
      "2       0.50      0.623   0.551  0.585\n",
      "3       0.55      0.653   0.446  0.530\n",
      "4       0.60      0.685   0.331  0.446\n",
      "5       0.65      0.721   0.219  0.335\n",
      "6       0.70      0.759   0.121  0.208\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.55, 0.6, 0.65, 0.7]\n",
    "\n",
    "results = []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append({\n",
    "        \"Threshold\": t,\n",
    "        \"Precision\": round(prec, 3),\n",
    "        \"Recall\": round(rec, 3),\n",
    "        \"F1\": round(f1, 3)\n",
    "    })\n",
    "\n",
    "df_thresh = pd.DataFrame(results)\n",
    "print(df_thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc37a77-171f-4714-a3d2-7dacc27b38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3292\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Threshold=0.30 → Precision=0.486, Recall=0.917, F1=0.636, AUC=0.747\n",
      "Threshold=0.40 → Precision=0.527, Recall=0.843, F1=0.648, AUC=0.747\n",
      "Threshold=0.50 → Precision=0.570, Recall=0.728, F1=0.639, AUC=0.747\n",
      "Threshold=0.60 → Precision=0.622, Recall=0.557, F1=0.588, AUC=0.747\n",
      "Threshold=0.70 → Precision=0.688, Recall=0.319, F1=0.436, AUC=0.747\n"
     ]
    }
   ],
   "source": [
    "df[\"콜레스테롤비율\"] = df[\"총콜레스테롤\"] / (df[\"HDL콜레스테롤\"] + 1e-6)\n",
    "df[\"AST_ALT비율\"] = df[\"혈청지오티(AST)\"] / (df[\"혈청지피티(ALT)\"] + 1e-6)\n",
    "\n",
    "\n",
    "target = \"고혈당_분석용\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"고혈당_분석용\",\"고혈당_서비스용\",\"식전혈당(공복혈당)\",\"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"성별코드\",\"흡연상태\",\"음주여부\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0:1, 1:scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"Threshold={t:.2f} → Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a2b509a-7727-4725-8450-7dd7e1c14614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 317970, number of negative: 482030\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3606\n",
      "[LightGBM] [Info] Number of data points in the train set: 800000, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Threshold=0.30 → Precision=0.487, Recall=0.917, F1=0.636, AUC=0.747\n",
      "Threshold=0.40 → Precision=0.527, Recall=0.842, F1=0.648, AUC=0.747\n",
      "Threshold=0.50 → Precision=0.570, Recall=0.727, F1=0.639, AUC=0.747\n",
      "Threshold=0.60 → Precision=0.622, Recall=0.556, F1=0.587, AUC=0.747\n",
      "Threshold=0.70 → Precision=0.688, Recall=0.319, F1=0.436, AUC=0.747\n"
     ]
    }
   ],
   "source": [
    "df[\"BMI\"] = df[\"체중(5kg단위)\"] * 5 / ((df[\"신장(5cm단위)\"]*5 / 100) ** 2)  \n",
    "df[\"혈압차\"] = df[\"수축기혈압\"] - df[\"이완기혈압\"]\n",
    "df[\"콜레스테롤비율\"] = df[\"총콜레스테롤\"] / (df[\"HDL콜레스테롤\"] + 1e-6)\n",
    "df[\"AST_ALT비율\"] = df[\"혈청지오티(AST)\"] / (df[\"혈청지피티(ALT)\"] + 1e-6)\n",
    "\n",
    "target = \"고혈당_분석용\"\n",
    "y = df[target].astype(int)\n",
    "\n",
    "exclude_cols = [\"고혈당_분석용\",\"고혈당_서비스용\",\"식전혈당(공복혈당)\",\"risk_level\"]\n",
    "X = df.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "cat_cols = [\"성별코드\",\"흡연상태\",\"음주여부\"]\n",
    "X = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "X = X.astype(\"float32\").fillna(0)\n",
    "\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "neg, pos = (y_train==0).sum(), (y_train==1).sum()\n",
    "scale = neg / pos if pos > 0 else 1\n",
    "\n",
    "model = LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight={0:1, 1:scale},\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "for t in thresholds:\n",
    "    y_pred = (y_pred_proba >= t).astype(int)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"Threshold={t:.2f} → Precision={prec:.3f}, Recall={rec:.3f}, F1={f1:.3f}, AUC={auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce7ae890-92b3-49b0-b780-1d6238931f18",
   "metadata": {},
   "source": [
    "Threshold=0.40 → Precision=0.527, Recall=0.841, F1=0.648, AUC=0.747\n",
    "최적의 결과 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4083eff0-113c-426a-9d2b-60f2d01af2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "meta = {\n",
    "    \"features\": X.columns.tolist(),  \n",
    "    \"median_values\": X.median().to_dict() \n",
    "}\n",
    "\n",
    "with open(\"glucose_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump((model, meta), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8c37c-1244-44ad-bbab-48447aead0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
